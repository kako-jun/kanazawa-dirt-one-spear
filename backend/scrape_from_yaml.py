#!/usr/bin/env python3
"""
YAMLã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã¨çµæœã‚’å–å¾—
"""
import sys
import yaml
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from app.scrapers.nar_scraper import NARScraper
from app.database import init_db, SessionLocal
from app import crud


def load_schedule(yaml_file: str) -> dict:
    """YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    with open(yaml_file, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def scrape_from_schedule(yaml_file: str, fetch_results: bool = False):
    """
    YAMLã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

    Args:
        yaml_file: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«YAMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        fetch_results: ãƒ¬ãƒ¼ã‚¹çµæœã‚‚å–å¾—ã™ã‚‹ã‹ï¼ˆTrueã®å ´åˆï¼‰
    """
    print("=" * 60)
    print("YAML based scraping")
    print("=" * 60)
    print(f"YAMLãƒ•ã‚¡ã‚¤ãƒ«: {yaml_file}")
    print(f"çµæœå–å¾—: {'ON' if fetch_results else 'OFF'}")
    print("=" * 60 + "\n")

    # YAMLã‚’èª­ã¿è¾¼ã¿
    schedule = load_schedule(yaml_file)
    year = schedule['year']
    track_name = schedule['track_name']
    track_code = schedule['track_code']
    dates = schedule['schedule']

    print(f"å¯¾è±¡: {year}å¹´ {track_name}")
    print(f"é–‹å‚¬æ—¥æ•°: {len(dates)}æ—¥\n")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    init_db()
    db = SessionLocal()

    scraper = NARScraper()

    # çµ±è¨ˆ
    stats = {
        'total_dates': len(dates),
        'races_scraped': 0,
        'races_saved': 0,
        'results_scraped': 0,
        'results_saved': 0,
        'failed': 0,
        'skipped': 0,
    }

    # å„é–‹å‚¬æ—¥ã‚’å‡¦ç†
    for i, date_info in enumerate(dates, 1):
        date_str = date_info['date']
        weekday = date_info['weekday']
        race_date = datetime.strptime(date_str, '%Y-%m-%d')

        print(f"[{i}/{len(dates)}] {date_str} ({weekday})")
        print("-" * 60)

        # ãƒ¬ãƒ¼ã‚¹ç•ªå·ä¸€è¦§ã‚’å–å¾—
        race_numbers = scraper.get_race_list(race_date)
        if not race_numbers:
            print(f"  âš ï¸  ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            stats['failed'] += 1
            print()
            continue

        print(f"  ãƒ¬ãƒ¼ã‚¹æ•°: {len(race_numbers)}R")

        # å„ãƒ¬ãƒ¼ã‚¹ã‚’å‡¦ç†
        for race_no in race_numbers:
            race_id = f"race_{race_date.strftime('%Y%m%d')}_{race_no:02d}"

            try:
                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                existing = crud.get_race(db, race_id)
                if existing:
                    print(f"    R{race_no:2d}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰", end="")
                    stats['skipped'] += 1

                    # çµæœå–å¾—ãƒ¢ãƒ¼ãƒ‰ã§ã€ã¾ã çµæœãŒãªã‘ã‚Œã°å–å¾—
                    if fetch_results:
                        existing_result = crud.get_result(db, race_id)
                        if not existing_result:
                            result_data = scraper.scrape_result(race_date, race_no)
                            if result_data:
                                # TODO: çµæœã‚’DBã«ä¿å­˜
                                print(" â†’ çµæœå–å¾—âœ…")
                                stats['results_scraped'] += 1
                            else:
                                print(" â†’ çµæœãªã—")
                        else:
                            print()
                    else:
                        print()
                    continue

                # å‡ºé¦¬è¡¨ã‚’å–å¾—
                race = scraper.scrape_race(race_date, race_no)
                if not race:
                    print(f"    R{race_no:2d}: âŒ å–å¾—å¤±æ•—")
                    stats['failed'] += 1
                    continue

                stats['races_scraped'] += 1

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                crud.create_race(db, race)
                stats['races_saved'] += 1
                horses_count = len(race.entries)
                print(f"    R{race_no:2d}: âœ… {race.name[:20]} ({horses_count}é ­)", end="")

                # çµæœå–å¾—ãƒ¢ãƒ¼ãƒ‰
                if fetch_results:
                    result_data = scraper.scrape_result(race_date, race_no)
                    if result_data:
                        # TODO: çµæœã‚’DBã«ä¿å­˜ã™ã‚‹æ©Ÿèƒ½ã‚’å®Ÿè£…
                        print(f" + çµæœâœ…")
                        stats['results_scraped'] += 1
                    else:
                        print(f" - çµæœãªã—")
                else:
                    print()

            except Exception as e:
                print(f"    R{race_no:2d}: âŒ ã‚¨ãƒ©ãƒ¼ - {e}")
                stats['failed'] += 1

        print()

    scraper.close()
    db.close()

    # æœ€çµ‚çµ±è¨ˆ
    print("=" * 60)
    print("å®Œäº†")
    print("=" * 60)
    print(f"é–‹å‚¬æ—¥æ•°:          {stats['total_dates']}æ—¥")
    print(f"å–å¾—ãƒ¬ãƒ¼ã‚¹æ•°:      {stats['races_scraped']}ãƒ¬ãƒ¼ã‚¹")
    print(f"ä¿å­˜ãƒ¬ãƒ¼ã‚¹æ•°:      {stats['races_saved']}ãƒ¬ãƒ¼ã‚¹")
    print(f"ã‚¹ã‚­ãƒƒãƒ—:          {stats['skipped']}ãƒ¬ãƒ¼ã‚¹")
    if fetch_results:
        print(f"å–å¾—çµæœæ•°:        {stats['results_scraped']}ä»¶")
        print(f"ä¿å­˜çµæœæ•°:        {stats['results_saved']}ä»¶")
    print(f"å¤±æ•—ãƒ»ã‚¨ãƒ©ãƒ¼:      {stats['failed']}ä»¶")
    print("=" * 60)

    if stats['races_saved'] > 0:
        print(f"\nğŸ‰ {stats['races_saved']}ãƒ¬ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='YAMLã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—')
    parser.add_argument('yaml_file', help='ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«YAMLãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: data/2025_schedule.yamlï¼‰')
    parser.add_argument('--results', action='store_true', help='ãƒ¬ãƒ¼ã‚¹çµæœã‚‚å–å¾—ã™ã‚‹')

    args = parser.parse_args()

    try:
        scrape_from_schedule(args.yaml_file, fetch_results=args.results)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
